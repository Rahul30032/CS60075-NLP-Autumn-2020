{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP_Assignment_2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zyJ25uz0kSaw"},"source":["# Assignment 2 on Natural Language Processing\n","\n","### Date : 15th Sept, 2020\n","\n","### Instructor : Prof. Sudeshna Sarkar\n","\n","### Teaching Assistants : Alapan Kuila, Aniruddha Roy, Anusha Potnuru, Uppada Vishnu"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Ao1nhg9RknmF"},"source":["The central idea of this assignment is to make you familiar with programming in python and also the language modelling task of natural language processing using the python.\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"stk58juYkzEr"},"source":["**Dataset:** \n","\n"," Use the text file provided along."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"rT6byv49kdmo","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600787654237,"user_tz":-330,"elapsed":1234,"user":{"displayName":"Rahul Ramesh","photoUrl":"","userId":"10743772275648160072"}},"outputId":"997d6580-1197-4464-8829-079d6e74babd"},"source":["# read dataset\n","#mounting corpus txt file from drive\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n"],"execution_count":163,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-ZqmIEDk4axw","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600787654240,"user_tz":-330,"elapsed":1220,"user":{"displayName":"Rahul Ramesh","photoUrl":"","userId":"10743772275648160072"}}},"source":["dataset = open(r\"/content/drive/My Drive/CS60075-NLP/assgnmt_2/corpus.txt\",\"r+\")\n","corpus = []\n","for line in dataset:\n","  corpus.append(line)\n","\n","# making string corpus ready from opening the txt file from drive. First lines are read into a list and then concatenated with a '\"\"'\n","textdata = \"\".join(corpus)\n","# print(dataset)\n","# print(textdata)"],"execution_count":164,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"SRGqKaDn1pJy"},"source":["\n","Preprocess the data"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"C1OtHn6B1oc2","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1600787654245,"user_tz":-330,"elapsed":1212,"user":{"displayName":"Rahul Ramesh","photoUrl":"","userId":"10743772275648160072"}},"outputId":"b2d63bf1-3c2c-41d6-a040-61cda4143a0b"},"source":["import re  \n","import nltk\n","nltk.download('punkt') # For tokenizers\n","from nltk.tokenize import word_tokenize,sent_tokenize\n","sent_tokens = sent_tokenize(textdata)\n","# sent_tokens[:2] #checking if sentence tokenized perfectly"],"execution_count":165,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Q6Mu8RN9T5LZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600787654683,"user_tz":-330,"elapsed":1637,"user":{"displayName":"Rahul Ramesh","photoUrl":"","userId":"10743772275648160072"}},"outputId":"e9bce3d4-6fa2-48a7-dfe8-7ee96e5974ec"},"source":["for i in range(len(sent_tokens)):\n","  sent_tokens[i] = sent_tokens[i].lower()  #lowrcasing sentences\n","  sent_tokens[i] = re.sub(r\"[^a-zA-Z0-9 ]+\", \"\", sent_tokens[i])  #for removing punctutation\n","  sent_tokens[i] = word_tokenize(sent_tokens[i]) #word tokenizing sentences\n","sent_tokens[:1] #for checking if preprocessing done properly "],"execution_count":166,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['chapter', 'i']]"]},"metadata":{"tags":[]},"execution_count":166}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"YDL7yfpXkMRS"},"source":["### Task: In this sub-task, you are expected to carry out the following tasks:\n","\n","1. **Create the following language models** on the training corpus: <br>\n","    i.   Unigram <br>\n","    ii.  Bigram <br>\n","    iii. Trigram <br>\n","    iv.  Fourgram <br>\n","\n","2. **List the top 5 bigrams, trigrams, four-grams (with and without Add-1 smoothing).**\n","(Note: Please remove those which contain only articles, prepositions, determiners. For Example: “of the”, “in a”, etc)."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"u3oIulBikPua","colab":{"base_uri":"https://localhost:8080/","height":972},"executionInfo":{"status":"ok","timestamp":1600787654685,"user_tz":-330,"elapsed":1624,"user":{"displayName":"Rahul Ramesh","photoUrl":"","userId":"10743772275648160072"}},"outputId":"2225ac20-edd2-49cf-a0a5-645ed1772702"},"source":["#Write code\n","\n","from nltk.util import ngrams\n","unigrams=[]\n","bigrams=[]\n","trigrams=[]\n","fourgrams=[]\n","\n","for content in sent_tokens:\n","    unigrams.extend(content)\n","    bigrams.extend(ngrams(content,2))\n","    ##similar for trigrams and fourgrams\n","    trigrams.extend(ngrams(content,3))\n","    fourgrams.extend(ngrams(content,4))\n","\n","#fdistxy : x = n from ngram and y = 1 means without stopword removal case\n","fdist11 = nltk.FreqDist(unigrams)\n","k = Counter(fdist11)\n","ten = k.most_common(10)\n","print(\"top 10 unigrams without smoothing without stopwords removal\\n\")\n","for i in ten:\n","  print(i[0],\" \",i[1],\" \")\n","fdist21 = nltk.FreqDist(bigrams)\n","k = Counter(fdist21)\n","ten = k.most_common(10)\n","print(\"\\ntop 10 bigrams without smoothing without stopwords removal\\n\")\n","for i in ten:\n","  print(i[0],\" \",i[1],\" \")\n","fdist31 = nltk.FreqDist(trigrams)\n","k = Counter(fdist31)\n","ten = k.most_common(10)\n","print(\"\\ntop 10 trigrams without smoothing without stopwords removal\\n\")\n","for i in ten:\n","  print(i[0],\" \",i[1],\" \")\n","fdist41 = nltk.FreqDist(fourgrams)\n","k = Counter(fdist41)\n","ten = k.most_common(10)\n","print(\"\\ntop 10 fourgrams without smoothing without stopwords removal\\n\")\n","for i in ten:\n","  print(i[0],\" \",i[1],\" \")"],"execution_count":167,"outputs":[{"output_type":"stream","text":["top 10 unigrams without smoothing without stopwords removal\n","\n","the   1630  \n","and   844  \n","to   721  \n","a   627  \n","she   537  \n","it   526  \n","of   508  \n","said   462  \n","i   400  \n","alice   385  \n","\n","top 10 bigrams without smoothing without stopwords removal\n","\n","('said', 'the')   209  \n","('of', 'the')   130  \n","('said', 'alice')   115  \n","('in', 'a')   97  \n","('and', 'the')   80  \n","('in', 'the')   78  \n","('it', 'was')   74  \n","('to', 'the')   69  \n","('the', 'queen')   65  \n","('as', 'she')   61  \n","\n","top 10 trigrams without smoothing without stopwords removal\n","\n","('the', 'mock', 'turtle')   51  \n","('the', 'march', 'hare')   30  \n","('said', 'the', 'king')   29  \n","('the', 'white', 'rabbit')   21  \n","('said', 'the', 'hatter')   21  \n","('said', 'to', 'herself')   19  \n","('said', 'the', 'mock')   19  \n","('said', 'the', 'caterpillar')   18  \n","('she', 'went', 'on')   17  \n","('she', 'said', 'to')   17  \n","\n","top 10 fourgrams without smoothing without stopwords removal\n","\n","('said', 'the', 'mock', 'turtle')   19  \n","('she', 'said', 'to', 'herself')   16  \n","('a', 'minute', 'or', 'two')   11  \n","('said', 'the', 'march', 'hare')   8  \n","('will', 'you', 'wont', 'you')   8  \n","('said', 'alice', 'in', 'a')   7  \n","('as', 'well', 'as', 'she')   6  \n","('well', 'as', 'she', 'could')   6  \n","('in', 'a', 'great', 'hurry')   6  \n","('in', 'a', 'tone', 'of')   6  \n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: generator 'ngrams' raised StopIteration\n","  \n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: generator 'ngrams' raised StopIteration\n","  del sys.path[0]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vARsvSfynePr","colab":{"base_uri":"https://localhost:8080/","height":937},"executionInfo":{"status":"ok","timestamp":1600787655067,"user_tz":-330,"elapsed":1991,"user":{"displayName":"Rahul Ramesh","photoUrl":"","userId":"10743772275648160072"}},"outputId":"3a121c82-8055-4866-c39e-c482f5ec3bf8"},"source":["#stopwords = code for downloading stop words through nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","stop_words = set(stopwords.words('english'))\n","# \n","from collections import Counter #for counting no of occurences of unique ngrams\n","#print top 10 unigrams, bigrams after removing stopwords\n","uni_processed = [p for p in unigrams if p not in stop_words]\n","\n","#fdistxy: x = n from ngram y = 2 means without smoothing and with stopwords removal\n","fdist12 = nltk.FreqDist(uni_processed)\n","k = Counter(fdist12)\n","ten = k.most_common(10)\n","print(\"printing top 10 unigrams post removing stopwords with their count\\n\")\n","for i in ten:\n","  print(i[0],\" \",i[1],\" \")\n","#print top 10 bigrams, trigrams, fourgrams after removing stopwords\n","bi_processed = []\n","tri_processed = []\n","four_processed = []\n","for p in bigrams:\n","  for t in p:\n","    if t not in stop_words:\n","      bi_processed.append(p)\n","\n","fdist22 = nltk.FreqDist(bi_processed)\n","l= Counter(fdist22)\n","bi_ten = l.most_common(10)\n","print(\"\\nprinting top 10 bigrams post removing stopwords with their count\\n\")\n","for i in bi_ten:\n","  print(i[0],\" \",i[1],\" \")\n","\n","for p in trigrams:\n","  for t in p:\n","    if t not in stop_words:\n","      tri_processed.append(p)\n","\n","fdist32 = nltk.FreqDist(tri_processed)\n","k = Counter(fdist32)\n","tri_ten = k.most_common(10)\n","print(\"\\nprinting top 10 trigrams post removing stopwords with their count\\n\")\n","for i in tri_ten:\n","  print(i[0],\" \",i[1],\" \")\n","\n","for p in fourgrams:\n","  for t in p:\n","    if t not in stop_words:\n","      four_processed.append(p)\n","\n","fdist42 = nltk.FreqDist(four_processed)\n","k = Counter(fdist42)\n","four_ten = k.most_common(10)\n","print(\"\\nprinting top 10 fourgrams post removing stopwords with their count\\n\")\n","for i in four_ten:\n","  print(i[0],\" \",i[1],\" \")\n"],"execution_count":168,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","printing top 10 unigrams post removing stopwords with their count\n","\n","said   462  \n","alice   385  \n","little   128  \n","one   101  \n","like   85  \n","know   85  \n","would   83  \n","went   83  \n","could   77  \n","thought   74  \n","\n","printing top 10 bigrams post removing stopwords with their count\n","\n","('said', 'alice')   230  \n","('said', 'the')   209  \n","('mock', 'turtle')   108  \n","('the', 'queen')   65  \n","('march', 'hare')   62  \n","('the', 'king')   60  \n","('a', 'little')   59  \n","('the', 'mock')   53  \n","('the', 'gryphon')   53  \n","('thought', 'alice')   52  \n","\n","printing top 10 trigrams post removing stopwords with their count\n","\n","('the', 'mock', 'turtle')   102  \n","('the', 'march', 'hare')   60  \n","('said', 'the', 'king')   58  \n","('the', 'white', 'rabbit')   42  \n","('said', 'the', 'hatter')   42  \n","('said', 'the', 'mock')   38  \n","('said', 'the', 'caterpillar')   36  \n","('said', 'the', 'gryphon')   34  \n","('said', 'the', 'duchess')   30  \n","('said', 'the', 'cat')   28  \n","\n","printing top 10 fourgrams post removing stopwords with their count\n","\n","('said', 'the', 'mock', 'turtle')   57  \n","('said', 'the', 'march', 'hare')   24  \n","('a', 'minute', 'or', 'two')   22  \n","('she', 'said', 'to', 'herself')   16  \n","('the', 'little', 'golden', 'key')   15  \n","('the', 'poor', 'little', 'thing')   15  \n","('said', 'alice', 'in', 'a')   14  \n","('well', 'as', 'she', 'could')   12  \n","('in', 'a', 'great', 'hurry')   12  \n","('the', 'march', 'hare', 'said')   12  \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ioc1xNjmnim-"},"source":["# Applying Smoothing\n","\n","\n","Assume additional training data in which each possible N-gram occurs exactly once and adjust estimates.\n","\n","> ### $ Probability(ngram) = \\frac{Count(ngram)+1}{ N\\, +\\, V} $\n","\n","N: Total number of N-grams <br>\n","V: Number of unique N-grams\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"grh4sO0Yns4V","colab":{"base_uri":"https://localhost:8080/","height":920},"executionInfo":{"status":"ok","timestamp":1600787655069,"user_tz":-330,"elapsed":1980,"user":{"displayName":"Rahul Ramesh","photoUrl":"","userId":"10743772275648160072"}},"outputId":"1c8fd34b-4216-4406-fd1c-8e0c195ef6d9"},"source":["#You are to perform Add-1 smoothing here:\n","uni_prob = {}\n","bi_prob = {}\n","tri_prob={}\n","four_prob = {}\n","V1 = len(fdist11.keys())   #V: Number of unique 1-grams\n","N1 = len(unigrams)         #N: Total number of 1-grams\n","V2 = len(fdist21 .keys())  #V: Number of unique 2-grams\n","N2 = len(bigrams)          #N: Total number of 2-grams\n","V3 = len(fdist31.keys())   #V: Number of unique 3-grams\n","N3 = len(trigrams)         #N: Total number of 3-grams\n","V4 = len(fdist41.keys())   #V: Number of unique 4-grams\n","N4 = len(fourgrams)        #N: Total number of 4-grams\n","\n","for key in fdist11.keys():\n","  uni_prob[key] = (fdist11[key]+1)/(N1+V1)\n","#write similar code for bigram, trigram and fourgrams\n","for key in fdist21.keys():\n","  bi_prob[key] = (fdist21[key]+1)/(N2+V2)\n","for key in fdist31.keys():\n","  tri_prob[key] = (fdist31[key]+1)/(N3+V3)\n","for key in fdist41.keys():\n","  four_prob[key] = (fdist41[key]+1)/(N4+V4)\n","#Print top 10 unigram, bigram, trigram, fourgram after smoothing\n","k = Counter(uni_prob)\n","ten12 = k.most_common(10)\n","print(\"\\nprinting top 10 unigrams post smoothing without removing stopwords with their count\\n\")\n","for i in ten12:\n","  print(i[0],\" \",i[1],\" \")\n","l = Counter(bi_prob)\n","ten22 = l.most_common(10)\n","print(\"\\nprinting top 10 bigrams post smoothing without removing stopwords with their count\\n\")\n","for i in ten22:\n","  print(i[0],\" \",i[1],\" \")\n","m = Counter(tri_prob)\n","ten32 = m.most_common(10)\n","print(\"\\nprinting top 10 trigrams post smoothing without removing stopwords with their count\\n\")\n","for i in ten32:\n","  print(i[0],\" \",i[1],\" \")\n","n = Counter(four_prob)\n","ten42 = n.most_common(10)\n","print(\"\\nprinting top 10 fourgrams post smoothing without removing stopwords with their count\\n\")\n","for i in ten42:\n","  print(i[0],\" \",i[1],\" \")"],"execution_count":169,"outputs":[{"output_type":"stream","text":["\n","printing top 10 unigrams post smoothing without removing stopwords with their count\n","\n","the   0.05598846589543785  \n","and   0.029006899866122  \n","to   0.024784593731763414  \n","a   0.021557790669733273  \n","she   0.018468298376300162  \n","it   0.018090693762658337  \n","of   0.017472795303971715  \n","said   0.0158937214651059  \n","i   0.01376540455185198  \n","alice   0.013250489169613126  \n","\n","printing top 10 bigrams post smoothing without removing stopwords with their count\n","\n","('said', 'the')   0.005338892561143032  \n","('of', 'the')   0.0033304520262368432  \n","('said', 'alice')   0.00294910255758377  \n","('in', 'a')   0.0024914831952000814  \n","('and', 'the')   0.002059287130726598  \n","('in', 'the')   0.002008440534906188  \n","('it', 'was')   0.0019067473432653685  \n","('to', 'the')   0.0017796308537143437  \n","('the', 'queen')   0.0016779376620735243  \n","('as', 'she')   0.0015762444704327046  \n","\n","printing top 10 trigrams post smoothing without removing stopwords with their count\n","\n","('the', 'mock', 'turtle')   0.0011425056026717053  \n","('the', 'march', 'hare')   0.0006811091092850551  \n","('said', 'the', 'king')   0.0006591378476952147  \n","('the', 'white', 'rabbit')   0.00048336775497649073  \n","('said', 'the', 'hatter')   0.00048336775497649073  \n","('said', 'to', 'herself')   0.0004394252317968098  \n","('said', 'the', 'mock')   0.0004394252317968098  \n","('said', 'the', 'caterpillar')   0.00041745397020696926  \n","('she', 'went', 'on')   0.0003954827086171288  \n","('she', 'said', 'to')   0.0003954827086171288  \n","\n","printing top 10 fourgrams post smoothing without removing stopwords with their count\n","\n","('said', 'the', 'mock', 'turtle')   0.00043410314290675465  \n","('she', 'said', 'to', 'herself')   0.00036898767147074146  \n","('a', 'minute', 'or', 'two')   0.0002604618857440528  \n","('said', 'the', 'march', 'hare')   0.00019534641430803959  \n","('will', 'you', 'wont', 'you')   0.00019534641430803959  \n","('said', 'alice', 'in', 'a')   0.00017364125716270185  \n","('as', 'well', 'as', 'she')   0.00015193610001736413  \n","('well', 'as', 'she', 'could')   0.00015193610001736413  \n","('in', 'a', 'great', 'hurry')   0.00015193610001736413  \n","('in', 'a', 'tone', 'of')   0.00015193610001736413  \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"k0GL40mQmmt4"},"source":["### Predict the next word using statistical language modelling\n","\n","Using the above bigram, trigram, and fourgram models that you just experimented with, **predict the next word(top 5 probable) given the previous n(=2, 3, 4)-grams** for the sentences below.\n","\n","For str1, str2, you are to predict the next  2 possible word sequences using your trained smoothed models. <br> \n","For example, for the string 'He looked very' the answers can be as below: \n",">     (1) 'He looked very' *anxiouxly* \n",">     (2) 'He looked very' *uncomfortable* "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MBWKo5_Fmnbg","colab":{},"executionInfo":{"status":"ok","timestamp":1600787655073,"user_tz":-330,"elapsed":1972,"user":{"displayName":"Rahul Ramesh","photoUrl":"","userId":"10743772275648160072"}}},"source":["str1 = 'after that alice said the'\n","str2 = 'alice felt so desperate that she was'"],"execution_count":170,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ext_nVn2mvZt","colab":{"base_uri":"https://localhost:8080/","height":972},"executionInfo":{"status":"ok","timestamp":1600787672303,"user_tz":-330,"elapsed":19191,"user":{"displayName":"Rahul Ramesh","photoUrl":"","userId":"10743772275648160072"}},"outputId":"9cdd7dea-efec-40ac-c031-51132d6e80da"},"source":["#write code\n","list1 = str1.split(\" \")\n","list2 = str2.split(\" \")\n","#tuples for n-1 grams\n","#gramxy x:n from ngram y : str number \n","gram21 = list1[-1:]\n","gram31 =tuple(list1[-2:])\n","gram41 = tuple(list1[-3:])\n","\n","gram22 = list2[-1:]\n","gram32 =tuple(list2[-2:])\n","gram42 = tuple(list2[-3:])\n","\n","#declaring dictionaries which will store probabilities of all unigrams to come as the next words\n","#probxy x = n from n gram y = str number\n","\n","prob21={}\n","prob31={}\n","prob41={}\n","prob22={}\n","prob32={}\n","prob42={}\n","\n","for key in fdist11.keys():\n","  gram = (gram21[0],)+(key,)   #adding key to form n-gram\n","  if gram in bigrams:\n","    prob21[key] = bi_prob[gram]/uni_prob[gram21[0]]  #calculating bigram probability str1\n","  gram = gram31+(key,)\n","  if gram in trigrams:\n","    prob31[key] = tri_prob[gram]/bi_prob[gram31]    #calculating trigram probability str1\n","  gram = gram41+(key,)\n","  if gram in fourgrams:\n","    prob41[key] = four_prob[gram]/tri_prob[gram41]  #calculating fourgram probability  str1\n","  gram = (gram22[0],)+ (key,)\n","  if gram in bigrams:\n","    prob22[key] = bi_prob[gram]/uni_prob[gram22[0]] #calculating bigram probability str2\n","  gram = gram32+(key,)\n","  if gram in trigrams:\n","    prob32[key] = tri_prob[gram]/bi_prob[gram32]   #calculating trigram probability str2 \n","  gram = gram42+(key,)\n","  if gram in fourgrams:\n","    prob42[key] = four_prob[gram]/tri_prob[gram42]  #calculating fourgram probability str2\n","\n","#fivexy x=n from ngram model y=str number\n","\n","o = Counter(prob21)\n","five21 = o.most_common(5)\n","print(\"\\nTop 5 prediction for str1 using bigram model with probability value\\n\")\n","for i in five21:\n","  print(i[0],\" \",i[1],\" \")\n","\n","\n","print(\"\\n\")\n","\n","o = Counter(prob31)\n","five31 = o.most_common(5)\n","print(\"\\nTop 5 prediction for str1 using trigram model with probability value\\n\")\n","for i in five31:\n","  print(i[0],\" \",i[1],\" \")\n","\n","\n","print(\"\\n\")\n","\n","o = Counter(prob41)\n","five41 = o.most_common(5)\n","print(\"\\nTop 5 prediction for str1 using fourgram model with probability value\\n\")\n","for i in five41:\n","  print(i[0],\" \",i[1],\" \")\n","\n","\n","print(\"\\n\")\n","\n","o = Counter(prob22)\n","five22 = o.most_common(5)\n","print(\"\\nTop 5 prediction for str2 using bigram model with probability value\\n\")\n","for i in five22:\n","  print(i[0],\" \",i[1],\" \")\n","\n","\n","print(\"\\n\")\n","\n","o = Counter(prob32)\n","five32 = o.most_common(5)\n","print(\"\\nTop 5 prediction for str2 using trigram model with probability value\\n\")\n","for i in five32:\n","  print(i[0],\" \",i[1],\" \")\n","\n","\n","print(\"\\n\")\n","\n","o = Counter(prob42)\n","five42 = o.most_common(5)\n","print(\"\\nTop 5 prediction for str2 using fourgram model with probability value\\n\")\n","for i in five42:\n","  print(i[0],\" \",i[1],\" \")\n","\n","print(\"\\n\")\n"],"execution_count":171,"outputs":[{"output_type":"stream","text":["\n","Top 5 prediction for str1 using bigram model with probability value\n","\n","queen   0.02996934520776446  \n","king   0.0276989402677823  \n","mock   0.024520373351807286  \n","gryphon   0.024520373351807286  \n","hatter   0.023612211375814426  \n","\n","\n","\n","Top 5 prediction for str1 using trigram model with probability value\n","\n","king   0.12345965762496938  \n","hatter   0.09053708225831088  \n","mock   0.08230643841664627  \n","caterpillar   0.07819111649581394  \n","gryphon   0.07407579457498163  \n","\n","\n","\n","Top 5 prediction for str1 using fourgram model with probability value\n","\n","\n","\n","\n","Top 5 prediction for str2 using bigram model with probability value\n","\n","a   0.06619942716622831  \n","the   0.04344337407783733  \n","not   0.026893517286280255  \n","that   0.02482478518733562  \n","going   0.02482478518733562  \n","\n","\n","\n","Top 5 prediction for str2 using trigram model with probability value\n","\n","now   0.10802720042184824  \n","quite   0.07716228601560587  \n","a   0.0617298288124847  \n","beginning   0.04629737160936353  \n","walking   0.04629737160936353  \n","\n","\n","\n","Top 5 prediction for str2 using fourgram model with probability value\n","\n","now   0.3292961741043005  \n","in   0.21953078273620033  \n","quite   0.21953078273620033  \n","dozing   0.21953078273620033  \n","walking   0.21953078273620033  \n","\n","\n"],"name":"stdout"}]}]}